{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "016ea90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b422129a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REFERENCE:\n",
    "# https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ea4e275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "785"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_set = np.load(\"fashion_train.npy\")\n",
    "test_data_set = np.load(\"fashion_test.npy\")\n",
    "\n",
    "\n",
    "# We have 10000 images in the training set\n",
    "len(train_data_set)\n",
    "\n",
    "# To find classification for each element, the last value in the set is:\n",
    "# 0 for T-shirt/top\n",
    "# 1 for Trousers\n",
    "# 2 for Pullover\n",
    "# 3 for Dress\n",
    "# 4 for Shirt\n",
    "len(train_data_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "414068a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to clean the array, as the last value is the classification. Let's first save these separately.\n",
    "\n",
    "train_classes = []\n",
    "\n",
    "for i in range(len(train_data_set)):\n",
    "    train_classes.append(train_data_set[i][784])\n",
    "      \n",
    "train_classes = np.array(train_classes)\n",
    "#train_classes = train_classes.reshape(-1, 1)\n",
    "\n",
    "\n",
    "test_classes = []\n",
    "\n",
    "for i in range(len(test_data_set)):\n",
    "    test_classes.append(test_data_set[i][784])\n",
    "      \n",
    "test_classes = np.array(test_classes)\n",
    "#test_classes = test_classes.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "957bff35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can delete the last element and have a cleaned training set that we can work with.\n",
    "\n",
    "cleaned_train_set = np.empty((len(train_data_set), 784), dtype=np.uint8)\n",
    "\n",
    "for i in range(len(train_data_set)):\n",
    "    temp = np.delete(train_data_set[i], 784)\n",
    "    cleaned_train_set[i] = temp\n",
    "    \n",
    "    \n",
    "cleaned_test_set = np.empty((len(test_data_set), 784), dtype=np.uint8)\n",
    "\n",
    "for i in range(len(test_data_set)):\n",
    "    temp = np.delete(test_data_set[i], 784)\n",
    "    cleaned_test_set[i] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c4393cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We reshape it so that the 1D array becomes a 2D 28x28 array. \n",
    "# We also need to save the number of channels in the second value for our CNN to work.\n",
    "#cleaned_train_set_reshaped = cleaned_train_set.reshape(-1, 28, 28)\n",
    "#cleaned_test_set_reshaped = cleaned_test_set.reshape(-1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c778ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We normalise the pixel values of the images to range from 0 to 1\n",
    "X_train = cleaned_train_set/255\n",
    "X_test = cleaned_test_set/255\n",
    "\n",
    "# Not used at the moment, but if we need to reshape our y to be (n, 1)\n",
    "y_train = train_classes.reshape(-1, 1)\n",
    "y_test = test_classes.reshape(-1, 1)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "stand_X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# Also apply standardization to test data\n",
    "stand_X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b258be5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to find the right number of PCA components, as we want the components to explain 90% of the variance in total.\n",
    "pca_components = len(stand_X_train[0])\n",
    "pca = PCA(pca_components)\n",
    "pca_train_result = pca.fit_transform(stand_X_train)\n",
    "\n",
    "\n",
    "pca_sum_list = [pca.explained_variance_ratio_[0]]\n",
    "i = 1\n",
    "\n",
    "while sum(pca_sum_list) < 0.90:\n",
    "    pca_sum_list.append(pca.explained_variance_ratio_[0+i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57dc3f5e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed data shape: (10000, 112)\n",
      "Explained variance ratio: [0.22645698 0.13673557 0.06381166 0.05046707 0.03064291 0.02924805\n",
      " 0.02197067 0.01880203 0.0147053  0.01416444 0.01298674 0.01232636\n",
      " 0.01005118 0.00914286 0.00886118 0.00844979 0.00800824 0.0074046\n",
      " 0.0071842  0.00678097 0.00637763 0.00625898 0.0059261  0.00557587\n",
      " 0.00524909 0.00501018 0.00489192 0.0047749  0.00451686 0.00424941\n",
      " 0.00419099 0.00403955 0.00393599 0.00388444 0.00379559 0.00362945\n",
      " 0.00357774 0.00351893 0.00333661 0.00324689 0.00315796 0.00312242\n",
      " 0.00298241 0.00291242 0.00277143 0.00273706 0.00264378 0.00258012\n",
      " 0.00245685 0.0024353 ]\n",
      "The sum of our PCA's explained variance: 0.9001461330930509\n"
     ]
    }
   ],
   "source": [
    "# Print the shapes and explained variance ratio\n",
    "pca_components = len(pca_sum_list)\n",
    "pca = PCA(pca_components)\n",
    "pca_train_result = pca.fit_transform(stand_X_train)\n",
    "pca_test_result = pca.transform(stand_X_test)\n",
    "\n",
    "print(\"Transformed data shape:\", pca_train_result.shape)\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_[:50])\n",
    "\n",
    "print(f'The sum of our PCA\\'s explained variance: {sum(pca.explained_variance_ratio_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "951ac2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "X, y = pca_train_result, train_classes\n",
    "X_test, y_test = pca_test_result, test_classes\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf39e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c99b78fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "# Assuming the labels are available as y_train and y_val\n",
    "# Convert labels to PyTorch tensors\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create TensorDataset\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ff75026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFNN(\n",
      "  (fc1): Linear(in_features=112, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FFNN, self).__init__()\n",
    "        # Simple 3-layer FFNN\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc3 = nn.Linear(hidden_dim // 2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))  # ReLU activation\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # No activation on the output (logits for classification)\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "input_dim = pca_train_result.shape[1]  # Number of PCA components\n",
    "hidden_dim = 256  # Number of hidden neurons\n",
    "output_dim = 5 # Number of classes (for classification)\n",
    "\n",
    "model = FFNN(input_dim, hidden_dim, output_dim)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ba073ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # Use for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c914515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.0223\n",
      "Epoch [2/20], Loss: 0.0299\n",
      "Epoch [3/20], Loss: 0.0164\n",
      "Epoch [4/20], Loss: 0.0466\n",
      "Epoch [5/20], Loss: 0.0161\n",
      "Epoch [6/20], Loss: 0.0219\n",
      "Epoch [7/20], Loss: 0.0159\n",
      "Epoch [8/20], Loss: 0.0082\n",
      "Epoch [9/20], Loss: 0.0046\n",
      "Epoch [10/20], Loss: 0.0087\n",
      "Epoch [11/20], Loss: 0.0037\n",
      "Epoch [12/20], Loss: 0.0013\n",
      "Epoch [13/20], Loss: 0.0009\n",
      "Epoch [14/20], Loss: 0.0008\n",
      "Epoch [15/20], Loss: 0.0007\n",
      "Epoch [16/20], Loss: 0.0010\n",
      "Epoch [17/20], Loss: 0.0006\n",
      "Epoch [18/20], Loss: 0.0005\n",
      "Epoch [19/20], Loss: 0.0004\n",
      "Epoch [20/20], Loss: 0.0004\n",
      "Test Accuracy: 84.84%\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        inputs, labels = batch\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()  # Zero gradients\n",
    "        loss.backward()        # Backpropagation\n",
    "        optimizer.step()       # Update weights\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf89ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70bf210a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('t-shirt/top', 'trousers', 'pullovers', 'dresses', 'shirts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a64387",
   "metadata": {},
   "source": [
    "#### Class accuracy for validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d211a05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class: t-shirt/top is 84.3 %\n",
      "Accuracy for class: trousers is 97.5 %\n",
      "Accuracy for class: pullovers is 89.0 %\n",
      "Accuracy for class: dresses is 91.9 %\n",
      "Accuracy for class: shirts is 70.2 %\n",
      "The combined accuracy of our model for validation data is: 86.6%\n"
     ]
    }
   ],
   "source": [
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in val_loader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "combined_accuracy = 0.0\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')\n",
    "    combined_accuracy += accuracy\n",
    "    \n",
    "combined_accuracy = combined_accuracy/5\n",
    "\n",
    "print(f'The combined accuracy of our model for validation data is: {combined_accuracy:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8e249b",
   "metadata": {},
   "source": [
    "#### Class accuracy for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c59de95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class: t-shirt/top is 82.3 %\n",
      "Accuracy for class: trousers is 96.3 %\n",
      "Accuracy for class: pullovers is 86.9 %\n",
      "Accuracy for class: dresses is 89.8 %\n",
      "Accuracy for class: shirts is 68.9 %\n",
      "The combined accuracy of our model for test data is: 84.8%\n"
     ]
    }
   ],
   "source": [
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "combined_accuracy = 0.0\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')\n",
    "    combined_accuracy += accuracy\n",
    "    \n",
    "    \n",
    "combined_accuracy = combined_accuracy/5\n",
    "\n",
    "print(f'The combined accuracy of our model for test data is: {combined_accuracy:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba463fa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
